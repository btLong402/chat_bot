import os

# Suppress noisy gRPC/ALTS logs (must be set before importing google libs)
os.environ.setdefault("GRPC_VERBOSITY", "ERROR")
os.environ.setdefault("GRPC_TRACE", "")

try:
    # Prefer the public Generative AI SDK. Avoid heavy setup at import time.
    import google.generativeai as _genai_pkg  # type: ignore
    _HAS_GENAI = True
except Exception:
    _genai_pkg = None
    _HAS_GENAI = False
try:
    from dotenv import load_dotenv
except Exception:
    # dotenv is optional; provide a no-op fallback so importing this module
    # doesn't fail in minimal environments.
    def load_dotenv():
        return None
from .memory import MemoryManager
from .retriever import RAGRetriever

load_dotenv()

import warnings

if not _HAS_GENAI:
    # don't raise at import time; let the application import the module.
    warnings.warn("google-genai package not installed. Install with: pip install google-genai", RuntimeWarning)

class GeminiBot:
    def __init__(self, name="GeminiBot", model="gemini-2.5-flash-lite", memory_file="chat_history.json"):
        self.name = name
        self.memory = MemoryManager(memory_file)
        self.retriever = RAGRetriever()
        if not _HAS_GENAI or _genai_pkg is None:
            raise RuntimeError("google-generativeai package not installed. Install with: pip install google-generativeai")
        api_key = os.getenv("GEMINI_API_KEY") or os.getenv("GOOGLE_API_KEY")
        if api_key:
            try:
                _genai_pkg.configure(api_key=api_key)
            except Exception as exc:
                raise RuntimeError(f"google-generativeai configure failed: {exc}")
        else:
            warnings.warn("GEMINI_API_KEY not set. Set it via environment or .env file.", RuntimeWarning)
        try:
            self._genai_model = _genai_pkg.GenerativeModel(model)
        except Exception as exc:
            raise RuntimeError(f"Failed to initialize GenerativeModel '{model}': {exc}")
        # maintain legacy attributes for compatibility with older calling code
        self.model = None
        self.chat = None

    def ask(self, message, use_rag=True, history_turns=10):
        context = ""
        if use_rag:
            results = self.retriever.retrieve(message)
            if results:
                context = "\n\n".join(results)

        history_snippets = []
        if history_turns:
            conversation = self.memory.get_conversation()
            if conversation:
                # Pull the last N entries from the stored conversation.
                for role, content in conversation[-history_turns:]:
                    history_snippets.append(f"{role}: {content}")
        history_block = "\n".join(history_snippets).strip() or "(kh√¥ng c√≥ l·ªãch s·ª≠ ph√π h·ª£p)"
        context_block = context.strip() or "(kh√¥ng c√≥ t√†i li·ªáu tham chi·∫øu ph√π h·ª£p)"

        prompt = f"""
B·∫°n l√† m·ªôt tr·ª£ l√Ω AI chuy√™n nghi·ªáp t√™n "{self.name}". 
Nhi·ªám v·ª• c·ªßa b·∫°n l√† cung c·∫•p c√°c c√¢u tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát, r√µ r√†ng, ch√≠nh x√°c, m·∫°ch l·∫°c v√† c√≥ th·ªÉ ki·ªÉm ch·ª©ng.

D·ªÆ LI·ªÜU ƒê·∫¶U V√ÄO
1) C√¢u h·ªèi hi·ªán t·∫°i c·ªßa ng∆∞·ªùi d√πng:
   "{message}"
2) L·ªãch s·ª≠ h·ªôi tho·∫°i g·∫ßn ƒë√¢y (vai tr√≤: n·ªôi dung):
   {history_block}
3) T√†i li·ªáu / th√¥ng tin tham chi·∫øu (t·ª´ h·ªá th·ªëng ho·∫∑c ngu·ªìn ƒë√°ng tin c·∫≠y):
   {context_block}

NGUY√äN T·∫ÆC CHUNG
- ∆Øu ti√™n tuy·ªát ƒë·ªëi cho **t√≠nh ch√≠nh x√°c** v√† **ƒë·ªô tin c·∫≠y**.
- Ch·ªâ s·ª≠ d·ª•ng th√¥ng tin c√≥ trong l·ªãch s·ª≠ tr√≤ chuy·ªán ho·∫∑c t√†i li·ªáu tham chi·∫øu; n·∫øu c·∫ßn suy lu·∫≠n, n√™u r√µ l·∫≠p lu·∫≠n.
- M·ªçi **t√≠nh to√°n** ƒë·ªÅu ph·∫£i ch√≠nh x√°c v√† hi·ªÉn th·ªã r√µ t·ª´ng b∆∞·ªõc (c√¥ng th·ª©c, gi·∫£ thi·∫øt, v√† k·∫øt qu·∫£).
- N·∫øu th√¥ng tin **thi·∫øu ho·∫∑c kh√¥ng ƒë·ªß**, h√£y ch·ªâ ra r√µ ph·∫ßn thi·∫øu v√† ƒë·ªÅ xu·∫•t h∆∞·ªõng x√°c minh / b·ªï sung.
- Khi tr√≠ch d·∫´n d·ªØ li·ªáu, lu√¥n **ƒë·ªëi ch·ª©ng v·ªõi ngu·ªìn tin c·∫≠y** (v√≠ d·ª•: b√†i b√°o khoa h·ªçc, t√†i li·ªáu ch√≠nh th·ªëng, website uy t√≠n).
- Tr√°nh kh·∫≥ng ƒë·ªãnh tuy·ªát ƒë·ªëi n·∫øu ch∆∞a ƒë·ªß d·ªØ li·ªáu; d√πng c√°c c·ª•m t·ª´ ‚Äúc√≥ th·ªÉ‚Äù, ‚Äú∆∞·ªõc t√≠nh‚Äù, ‚Äútheo d·ªØ li·ªáu hi·ªán c√≥‚Äù.

C·∫§U TR√öC TR·∫¢ L·ªúI
1. **T√≥m t·∫Øt ng·∫Øn:** tr·∫£ l·ªùi tr·ª±c ti·∫øp c√¢u h·ªèi trong 1‚Äì2 c√¢u.
2. **Ph√¢n t√≠ch chi ti·∫øt:**
   - L·∫≠p lu·∫≠n, b·∫±ng ch·ª©ng, ho·∫∑c d·ªØ li·ªáu li√™n quan.
   - N·∫øu c√≥ ph√©p t√≠nh, tr√¨nh b√†y c√¥ng th·ª©c v√† b∆∞·ªõc gi·∫£i.
   - N√™u r√µ gi·∫£ thi·∫øt v√† ph·∫°m vi √°p d·ª•ng.
3. **K·∫øt qu·∫£ trung t√¢m:** tr√¨nh b√†y con s·ªë ho·∫∑c k·∫øt lu·∫≠n r√µ r√†ng.
4. **Gi·ªõi h·∫°n v√† khuy·∫øn ngh·ªã:** n√™u h·∫°n ch·∫ø v√† h∆∞·ªõng m·ªü r·ªông ho·∫∑c x√°c minh.
5. **Ngu·ªìn tham kh·∫£o (n·∫øu c√≥):** ghi r√µ ngu·ªìn ho·∫∑c lo·∫°i t√†i li·ªáu ƒë√£ d√πng ƒë·ªÉ ƒë·ªëi ch·ª©ng.

Y√äU C·∫¶U ƒê·ªäNH D·∫†NG
- Tr·∫£ l·ªùi ho√†n to√†n b·∫±ng ti·∫øng Vi·ªát.
- Gi·ªØ vƒÉn phong h·ªçc thu·∫≠t, chu·∫©n x√°c, nh∆∞ng d·ªÖ hi·ªÉu.
- D√πng g·∫°ch ƒë·∫ßu d√≤ng ho·∫∑c ƒëo·∫°n vƒÉn ng·∫Øn ƒë·ªÉ r√µ r√†ng.
- Khi c√≥ m√£ ho·∫∑c c√¥ng th·ª©c, bao trong kh·ªëi m√£ markdown (```).
- Khi c√≥ s·ªë li·ªáu/bi·ªÉu ƒë·ªì, m√¥ t·∫£ ph∆∞∆°ng ph√°p t√≠nh ho·∫∑c gi·∫£ thi·∫øt ƒë√£ d√πng.

M·ª§C TI√äU T·ªîNG TH·ªÇ
Cung c·∫•p c√¢u tr·∫£ l·ªùi mang t√≠nh h·ªçc thu·∫≠t, ch√≠nh x√°c v·ªÅ m·∫∑t k·ªπ thu·∫≠t, minh b·∫°ch v·ªÅ l·∫≠p lu·∫≠n v√† ƒë∆∞·ª£c ƒë·ªëi ch·ª©ng b·∫±ng ngu·ªìn ƒë√°ng tin c·∫≠y tr∆∞·ªõc khi k·∫øt lu·∫≠n.
"""

        # Use google-generativeai to generate a response from the model.
        try:
            response = self._genai_model.generate_content(prompt)
            if hasattr(response, "text") and response.text:
                answer = response.text.strip()
            elif hasattr(response, "candidates") and response.candidates:
                parts = []
                for candidate in response.candidates:
                    content = getattr(candidate, "content", None)
                    if content and getattr(content, "parts", None):
                        for part in content.parts:
                            text = getattr(part, "text", None)
                            if text:
                                parts.append(text)
                answer = "\n".join(parts).strip() if parts else str(response).strip()
            else:
                answer = str(response).strip()
        except Exception as e:
            raise RuntimeError(f"Failed to get response from google-generativeai client: {e}")
        self.memory.add_message("user", message)
        self.memory.add_message("assistant", answer)
        return answer

    def clear_context(self):
        self.memory.clear_history()
        # No persistent chat object when using the `google-genai` client
        # in this simplified adapter; just clear conversation memory.
        self.chat = None
        return "üßπ B·ªô nh·ªõ h·ªôi tho·∫°i ƒë√£ ƒë∆∞·ª£c x√≥a!"

