import os

# Suppress noisy gRPC/ALTS logs (must be set before importing google libs)
os.environ.setdefault("GRPC_VERBOSITY", "ERROR")
os.environ.setdefault("GRPC_TRACE", "")

try:
    # Prefer the public Generative AI SDK. Avoid heavy setup at import time.
    import google.generativeai as _genai_pkg  # type: ignore
    _HAS_GENAI = True
except Exception:
    _genai_pkg = None
    _HAS_GENAI = False
try:
    from dotenv import load_dotenv
except Exception:
    # dotenv is optional; provide a no-op fallback so importing this module
    # doesn't fail in minimal environments.
    def load_dotenv():
        return None
from .memory import MemoryManager
from .retriever import RAGRetriever

load_dotenv()

import warnings

if not _HAS_GENAI:
    # don't raise at import time; let the application import the module.
    warnings.warn("google-genai package not installed. Install with: pip install google-genai", RuntimeWarning)

class GeminiBot:
    def __init__(self, name="GeminiBot", model="gemini-2.5-flash-lite", memory_file="chat_history.json"):
        self.name = name
        self.memory = MemoryManager(memory_file)
        self.retriever = RAGRetriever()
        if not _HAS_GENAI or _genai_pkg is None:
            raise RuntimeError("google-generativeai package not installed. Install with: pip install google-generativeai")
        api_key = os.getenv("GEMINI_API_KEY") or os.getenv("GOOGLE_API_KEY")
        if api_key:
            try:
                _genai_pkg.configure(api_key=api_key)
            except Exception as exc:
                raise RuntimeError(f"google-generativeai configure failed: {exc}")
        else:
            warnings.warn("GEMINI_API_KEY not set. Set it via environment or .env file.", RuntimeWarning)
        try:
            self._genai_model = _genai_pkg.GenerativeModel(model)
        except Exception as exc:
            raise RuntimeError(f"Failed to initialize GenerativeModel '{model}': {exc}")
        # maintain legacy attributes for compatibility with older calling code
        self.model = None
        self.chat = None

    def ask(self, message, use_rag=True, history_turns=10):
        context = ""
        if use_rag:
            results = self.retriever.retrieve(message)
            if results:
                context = "\n\n".join(results)

        history_snippets = []
        if history_turns:
            conversation = self.memory.get_conversation()
            if conversation:
                # Pull the last N entries from the stored conversation.
                for role, content in conversation[-history_turns:]:
                    history_snippets.append(f"{role}: {content}")
        history_block = "\n".join(history_snippets).strip() or "(kh√¥ng c√≥ l·ªãch s·ª≠ ph√π h·ª£p)"
        context_block = context.strip() or "(kh√¥ng c√≥ t√†i li·ªáu tham chi·∫øu ph√π h·ª£p)"

        prompt = f"""
        B·∫°n l√† tr·ª£ l√Ω AI chuy√™n nghi·ªáp t√™n {self.name}. Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát r√µ r√†ng, ch√≠nh x√°c v√† ƒë·∫ßy ƒë·ªß.

        D·ªÆ LI·ªÜU ƒê·∫¶U V√ÄO
        1. C√¢u h·ªèi hi·ªán t·∫°i c·ªßa ng∆∞·ªùi d√πng:
           "{message}"
        2. L·ªãch s·ª≠ tr√≤ chuy·ªán g·∫ßn ƒë√¢y (vai tr√≤: n·ªôi dung):
           {history_block}
        3. T√†i li·ªáu/th√¥ng tin tham chi·∫øu t·ª´ h·ªá th·ªëng:
           {context_block}

        H∆Ø·ªöNG D·∫™N TR·∫¢ L·ªúI
        - ∆Øu ti√™n s·ª± ch√≠nh x√°c; ch·ªâ s·ª≠ d·ª•ng th√¥ng tin c√≥ trong l·ªãch s·ª≠ v√† t√†i li·ªáu tham chi·∫øu.
        - N·∫øu c√¢u tr·∫£ l·ªùi c·∫ßn suy lu·∫≠n, h√£y n√™u l·∫≠p lu·∫≠n ng·∫Øn g·ªçn d·ª±a tr√™n d·ªØ li·ªáu.
        - N·∫øu thi·∫øu th√¥ng tin, h√£y n√™u r√µ ph·∫ßn ch∆∞a bi·∫øt v√† ƒë∆∞a ra g·ª£i √Ω ti·∫øp theo.
        - Gi·ªØ c·∫•u tr√∫c m·∫°ch l·∫°c v·ªõi ƒëo·∫°n vƒÉn ho·∫∑c g·∫°ch ƒë·∫ßu d√≤ng khi c·∫ßn.
        """
        # Use google-generativeai to generate a response from the model.
        try:
            response = self._genai_model.generate_content(prompt)
            if hasattr(response, "text") and response.text:
                answer = response.text.strip()
            elif hasattr(response, "candidates") and response.candidates:
                parts = []
                for candidate in response.candidates:
                    content = getattr(candidate, "content", None)
                    if content and getattr(content, "parts", None):
                        for part in content.parts:
                            text = getattr(part, "text", None)
                            if text:
                                parts.append(text)
                answer = "\n".join(parts).strip() if parts else str(response).strip()
            else:
                answer = str(response).strip()
        except Exception as e:
            raise RuntimeError(f"Failed to get response from google-generativeai client: {e}")
        self.memory.add_message("user", message)
        self.memory.add_message("assistant", answer)
        return answer

    def clear_context(self):
        self.memory.clear_history()
        # No persistent chat object when using the `google-genai` client
        # in this simplified adapter; just clear conversation memory.
        self.chat = None
        return "üßπ B·ªô nh·ªõ h·ªôi tho·∫°i ƒë√£ ƒë∆∞·ª£c x√≥a!"

