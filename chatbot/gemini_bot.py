import os

# Suppress noisy gRPC/ALTS logs (must be set before importing google libs)
os.environ.setdefault("GRPC_VERBOSITY", "ERROR")
os.environ.setdefault("GRPC_TRACE", "")

try:
    # Prefer the public Generative AI SDK. Avoid heavy setup at import time.
    import google.generativeai as _genai_pkg  # type: ignore
    _HAS_GENAI = True
except Exception:
    _genai_pkg = None
    _HAS_GENAI = False
try:
    from dotenv import load_dotenv
except Exception:
    # dotenv is optional; provide a no-op fallback so importing this module
    # doesn't fail in minimal environments.
    def load_dotenv():
        return None
from .memory import MemoryManager
from .retriever import RAGRetriever

load_dotenv()

import warnings

if not _HAS_GENAI:
    # don't raise at import time; let the application import the module.
    warnings.warn("google-genai package not installed. Install with: pip install google-genai", RuntimeWarning)

class GeminiBot:
    def __init__(self, name="La B√†n AI", model="gemini-2.5-flash-lite", memory_file="chat_history.json"):
        self.name = name
        self.memory = MemoryManager(memory_file)
        self.retriever = RAGRetriever()
        if not _HAS_GENAI or _genai_pkg is None:
            raise RuntimeError("google-generativeai package not installed. Install with: pip install google-generativeai")
        api_key = os.getenv("GEMINI_API_KEY") or os.getenv("GOOGLE_API_KEY")
        if api_key:
            try:
                _genai_pkg.configure(api_key=api_key)
            except Exception as exc:
                raise RuntimeError(f"google-generativeai configure failed: {exc}")
        else:
            warnings.warn("GEMINI_API_KEY not set. Set it via environment or .env file.", RuntimeWarning)
        try:
            self._genai_model = _genai_pkg.GenerativeModel(model)
        except Exception as exc:
            raise RuntimeError(f"Failed to initialize GenerativeModel '{model}': {exc}")
        # maintain legacy attributes for compatibility with older calling code
        self.model = None
        self.chat = None

    def ask(self, message, use_rag=True, history_turns=10):
        context = ""
        if use_rag:
            results = self.retriever.retrieve(message)
            if results:
                context = "\n\n".join(results)

        history_snippets = []
        if history_turns:
            conversation = self.memory.get_conversation()
            if conversation:
                # Pull the last N entries from the stored conversation.
                for role, content in conversation[-history_turns:]:
                    history_snippets.append(f"{role}: {content}")
        history_block = "\n".join(history_snippets).strip() or "(kh√¥ng c√≥ l·ªãch s·ª≠ ph√π h·ª£p)"
        context_block = context.strip() or "(kh√¥ng c√≥ t√†i li·ªáu tham chi·∫øu ph√π h·ª£p)"

        prompt = f"""
B·∫°n l√† m·ªôt tr·ª£ l√Ω AI chuy√™n nghi·ªáp t√™n "{self.name}". 
Nhi·ªám v·ª• c·ªßa b·∫°n l√† cung c·∫•p c√°c c√¢u tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát, r√µ r√†ng, ch√≠nh x√°c, m·∫°ch l·∫°c v√† c√≥ th·ªÉ ki·ªÉm ch·ª©ng.

D·ªÆ LI·ªÜU ƒê·∫¶U V√ÄO
1) C√¢u h·ªèi hi·ªán t·∫°i c·ªßa ng∆∞·ªùi d√πng:
   "{message}"
2) L·ªãch s·ª≠ h·ªôi tho·∫°i g·∫ßn ƒë√¢y (vai tr√≤: n·ªôi dung):
   {history_block}
3) T√†i li·ªáu / th√¥ng tin tham chi·∫øu (t·ª´ h·ªá th·ªëng ho·∫∑c ngu·ªìn ƒë√°ng tin c·∫≠y):
   {context_block}

NGUY√äN T·∫ÆC CHUNG
- ∆Øu ti√™n tuy·ªát ƒë·ªëi cho **t√≠nh ch√≠nh x√°c**, **ƒë·ªô tin c·∫≠y** v√† **m·ª•c ti√™u c·ªßa ng∆∞·ªùi d√πng**: tr·∫£ l·ªùi tr·ªçng t√¢m, kh√¥ng lan man.
- Ch·ªâ s·ª≠ d·ª•ng th√¥ng tin c√≥ trong l·ªãch s·ª≠ tr√≤ chuy·ªán ho·∫∑c t√†i li·ªáu tham chi·∫øu; n·∫øu c·∫ßn suy lu·∫≠n, n√™u r√µ l·∫≠p lu·∫≠n.
- M·ªçi **t√≠nh to√°n** ƒë·ªÅu ph·∫£i ch√≠nh x√°c v√† hi·ªÉn th·ªã r√µ t·ª´ng b∆∞·ªõc (c√¥ng th·ª©c, gi·∫£ thi·∫øt, v√† k·∫øt qu·∫£).
- N·∫øu th√¥ng tin **thi·∫øu ho·∫∑c kh√¥ng ƒë·ªß**, h√£y ch·ªâ ra r√µ ph·∫ßn thi·∫øu v√† ƒë·ªÅ xu·∫•t h∆∞·ªõng x√°c minh / b·ªï sung.
- Khi tr√≠ch d·∫´n d·ªØ li·ªáu, lu√¥n **ƒë·ªëi ch·ª©ng v·ªõi ngu·ªìn tin c·∫≠y** (v√≠ d·ª•: b√†i b√°o khoa h·ªçc, t√†i li·ªáu ch√≠nh th·ªëng, website uy t√≠n).
- Tr√°nh kh·∫≥ng ƒë·ªãnh tuy·ªát ƒë·ªëi n·∫øu ch∆∞a ƒë·ªß d·ªØ li·ªáu; d√πng c√°c c·ª•m t·ª´ ‚Äúc√≥ th·ªÉ‚Äù, ‚Äú∆∞·ªõc t√≠nh‚Äù, ‚Äútheo d·ªØ li·ªáu hi·ªán c√≥‚Äù.

C·∫§U TR√öC TR·∫¢ L·ªúI (ƒê√É T·ªêI ∆ØU)
1. üéØ **K·∫øt lu·∫≠n nhanh / Tr·∫£ l·ªùi ch√≠nh:**  
   - Tr·∫£ l·ªùi tr·ª±c ti·∫øp c√¢u h·ªèi ho·∫∑c k·∫øt qu·∫£ ch√≠nh m√† ng∆∞·ªùi d√πng mu·ªën bi·∫øt.  
   - C√≥ th·ªÉ bao g·ªìm con s·ªë ho·∫∑c khuy·∫øn ngh·ªã ng·∫Øn (n·∫øu ph√π h·ª£p).

2. üß≠ **Gi·∫£i th√≠ch ng·∫Øn g·ªçn ‚Äì V√¨ sao / C∆° s·ªü:**  
   - N√™u c√°c l√Ω do, d·ªØ ki·ªán, ho·∫∑c logic d·∫´n ƒë·∫øn k·∫øt lu·∫≠n tr√™n.  
   - N·∫øu c√≥ t√≠nh to√°n ho·∫∑c m√¥ h√¨nh, tr√¨nh b√†y d·∫°ng r√∫t g·ªçn, ch·ªâ ph·∫ßn c·ªët l√µi.

3. üßÆ **Chi ti·∫øt k·ªπ thu·∫≠t (ch·ªâ khi c·∫ßn):**  
   - C√°c b∆∞·ªõc t√≠nh, c√¥ng th·ª©c, ho·∫∑c minh ch·ª©ng h·ªó tr·ª£ k·∫øt qu·∫£.  
   - C√≥ th·ªÉ b·ªè qua n·∫øu kh√¥ng c·∫ßn cho m·ª•c ti√™u c·ªßa ng∆∞·ªùi d√πng.

4. ‚ö†Ô∏è **Gi·ªõi h·∫°n ho·∫∑c ƒëi·ªÅu ki·ªán √°p d·ª•ng:**  
   - Ch·ªâ ra c√°c gi·∫£ thi·∫øt, r√†ng bu·ªôc, ho·∫∑c ph·∫°m vi m√† k·∫øt qu·∫£ ƒë√∫ng.  
   - N·∫øu c√≥ r·ªßi ro/ngo·∫°i l·ªá, n√™u ng·∫Øn g·ªçn.

5. üìö **Ngu·ªìn / ƒê·ªëi ch·ª©ng (n·∫øu c√≥):**  
   - Li·ªát k√™ t√†i li·ªáu, d·ªØ li·ªáu, ho·∫∑c ph∆∞∆°ng ph√°p ƒë√£ d√πng ƒë·ªÉ ki·ªÉm ch·ª©ng.  

Y√äU C·∫¶U ƒê·ªäNH D·∫†NG
- Vi·∫øt b·∫±ng ti·∫øng Vi·ªát chu·∫©n, m·∫°ch l·∫°c, d·ªÖ hi·ªÉu cho ng∆∞·ªùi d√πng k·ªπ thu·∫≠t.  
- ∆Øu ti√™n ƒëo·∫°n vƒÉn ng·∫Øn ho·∫∑c g·∫°ch ƒë·∫ßu d√≤ng.  
- D√πng kh·ªëi m√£ markdown (```) cho c√¥ng th·ª©c, code ho·∫∑c s·ªë li·ªáu.  
- Kh√¥ng tr√¨nh b√†y d√†i d√≤ng, tr√°nh m√¥ t·∫£ l·∫∑p l·∫°i n·ªôi dung ƒë√£ r√µ.  

M·ª§C TI√äU
T·∫°o ra c√¢u tr·∫£ l·ªùi **ng·∫Øn ‚Äì ch√≠nh x√°c ‚Äì c√≥ gi√° tr·ªã th·ª±c t·∫ø cao**, gi√∫p ng∆∞·ªùi ƒë·ªçc hi·ªÉu ngay **k·∫øt qu·∫£ c·∫ßn bi·∫øt**, sau ƒë√≥ m·ªõi c√≥ th·ªÉ ƒë·ªçc ph·∫ßn chi ti·∫øt ƒë·ªÉ ki·ªÉm ch·ª©ng ho·∫∑c m·ªü r·ªông.
"""
        
        # Use google-generativeai to generate a response from the model.
        try:
            response = self._genai_model.generate_content(prompt)
            if hasattr(response, "text") and response.text:
                answer = response.text.strip()
            elif hasattr(response, "candidates") and response.candidates:
                parts = []
                for candidate in response.candidates:
                    content = getattr(candidate, "content", None)
                    if content and getattr(content, "parts", None):
                        for part in content.parts:
                            text = getattr(part, "text", None)
                            if text:
                                parts.append(text)
                answer = "\n".join(parts).strip() if parts else str(response).strip()
            else:
                answer = str(response).strip()
        except Exception as e:
            raise RuntimeError(f"Failed to get response from google-generativeai client: {e}")
        self.memory.add_message("user", message)
        self.memory.add_message("assistant", answer)
        return answer

    def clear_context(self):
        self.memory.clear_history()
        # No persistent chat object when using the `google-genai` client
        # in this simplified adapter; just clear conversation memory.
        self.chat = None
        return "üßπ B·ªô nh·ªõ h·ªôi tho·∫°i ƒë√£ ƒë∆∞·ª£c x√≥a!"

